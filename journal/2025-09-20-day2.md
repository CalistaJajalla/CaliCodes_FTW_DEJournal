# Journal — 2025-09-20 (Day 2)

## 1) What I learned (bullets, not prose)
- Why **joins** matter in SQL (INNER vs LEFT JOIN) and how they change what rows appear  
- The meaning of **primary keys (PKs)** and **foreign keys (FKs)** in relational databases  
- The steps of **normalization** (1NF, 2NF, 3NF, BCNF) and how they prevent messy anomalies  
- That **OLTP vs OLAP** is like two worlds: one for transactions, one for analytics  
- How **dimensional modeling (Kimball)** works, with **facts as verbs/measures** and **dimensions as nouns/descriptors**  
- Why **testing in dbt** is like checking if the data “makes sense” (not just if the pipeline runs)  

## 2) New vocabulary (define in your own words)
- **INNER JOIN** – Match only when both tables agree. Like “show me albums *with* an artist.”  
- **LEFT JOIN** – Keep everyone from the left table, even if no match exists. Like “all customers, even if they bought nothing.”  
- **Normalization** – Break down big messy tables into smaller, cleaner ones so updates don’t go haywire.  
- **1NF** – Each column should be atomic ⚛️, no lists hiding inside.  
- **2NF** – Attributes depend on the *whole* key 🔑, not just part of it.  
- **3NF** – Attributes depend only on the key 🔐, not on other attributes.  
- **BCNF** – Hardcore version of 3NF: every determinant must be a candidate key.  
- **OLTP** – Databases for speed and transactions (banking, orders).  
- **OLAP** – Databases for analysis and big-picture reports.  
- **dbt test** – Sanity checks for data, like “are emails unique?”  

## 3) Data Engineering mindset applied (what principles did I use?)
- **Don’t trust raw data blindly.** Normalization showed me how duplicate emails or genres could ruin insights.  
- **Think in trade-offs.** INNER JOIN vs LEFT JOIN isn’t just syntax — it’s a business decision on “who counts.”  
- **Facts vs Dimensions.** Treat measures and descriptors separately so analysis is clean and flexible.  
- **Testing is not fixing.** Data tests are like observability — you shine light on problems early, before they reach dashboards.  
  
![Alt text](../assets/NAN.jpg "NAN")

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- When testing SQL queries, I chose to **ORDER BY + LIMIT** instead of pulling everything.  
  - *Trade-off:* safer and faster for my laptop, but sometimes I might miss hidden rows or edge cases.  
- In joins, I practiced **LEFT JOIN** to catch customers with no purchases.  
  - Assumption: knowing “who’s missing” is as important as “who’s buying.”  
- I leaned on **GROUP BY + HAVING** instead of just WHERE when summarizing sales.  
  - *Trade-off:* WHERE feels simpler, but HAVING gives more power when working with aggregates.  

## 5) Open questions (things I still don’t get)
- How do professional data teams decide when to **denormalize** data for speed, instead of sticking with 3NF/BCNF?  
- Is there a rule of thumb for **which dimension tables** should get Slowly Changing Dimension (SCD2) tracking?  
- If **dbt test** only logs failures, how do teams prevent bad data from reaching dashboards in production?  

## 6) Next actions (small, doable steps)
- Write at least **3 JOIN queries** using the Chinook dataset, testing INNER vs LEFT JOIN.  
- Try building my **own mini star schema** (Fact + Dimensions) from Chinook.
  - Update: Tried this but it looks rough? I think there are better ways to optimize or normalize it.
- Run a **dbt test** with intentional “bad data” to see how failures are logged.  


## 7) Artifacts & links (code, queries, dashboards)

<details>
<summary>Click to expand SQL practice</summary>

```sql
-- Customers with or without invoices --
SELECT c.FirstName, c.LastName, i.InvoiceId
FROM Customer c
LEFT JOIN Invoice i
ON c.CustomerId = i.CustomerId;

-- Top 5 longest tracks --
SELECT Name, Milliseconds
FROM Track
ORDER BY Milliseconds DESC
LIMIT 5;

-- Countries with sales > $100 --
SELECT BillingCountry, SUM(Total) AS sales
FROM Invoice
GROUP BY BillingCountry
HAVING SUM(Total) > 100;
```
</details> 

### Mini reflection (3–5 sentences)
What surprised me? What would I do differently next time? What will I watch out for in production?

Today stretched me the most with normalization. It feels abstract, but the water analogy (1NF–BCNF = water safety levels) made it click.
I realized how easily bad design leads to contradictions (like duplicate emails).
Next time, I’ll practice designing tables from scratch instead of just querying them — so I see both sides (storage + analysis).

### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](../assets/date-everywhere-data.gif "date is everywhere!")
